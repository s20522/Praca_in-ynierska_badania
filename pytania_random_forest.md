#  Pytania do obrony pracy in偶ynierskiej - Model Random Forest

**Autor:** Heart Failure Research Team  
**Data:** 29 grudnia 2024  
**Cel:** Przygotowanie do obrony pracy in偶ynierskiej poprzez zrozumienie kluczowych aspekt贸w budowy, optymalizacji i ewaluacji modelu Random Forest.

---

## Wprowadzenie

Poni偶sza lista 30 pyta (wraz z odpowiedziami) zostaa stworzona, aby pom贸c w przygotowaniu si do obrony pracy in偶ynierskiej w sekcji dotyczcej modelu Random Forest. Pytania obejmuj zar贸wno podstawy teoretyczne algorytmu, jak i szczeg贸ow interpretacj wynik贸w uzyskanych w trakcie eksperyment贸w. Celem jest nie tylko "wykucie" odpowiedzi, ale przede wszystkim **gbokie zrozumienie** materiau.

## Spis Kategorii

1.  [**I. Teoria i Dziaanie Modelu (Pytania 1-7)**](#i-teoria-i-dziaanie-modelu)
2.  [**II. Przygotowanie Danych i Budowa Modelu (Pytania 8-14)**](#ii-przygotowanie-danych-i-budowa-modelu)
3.  [**III. Optymalizacja i Walidacja (Pytania 15-19)**](#iii-optymalizacja-i-walidacja)
4.  [**IV. Ewaluacja i Metryki (Pytania 20-26)**](#iv-ewaluacja-i-metryki)
5.  [**V. Interpretacja Wynik贸w i Wnioski (Pytania 27-30)**](#v-interpretacja-wynik贸w-i-wnioski)

---

## I. Teoria i Dziaanie Modelu

### Pytanie 1: Prosz wyjani, czym jest model Random Forest i na czym polega jego dziaanie?

> **Odpowied藕:**
> Random Forest (Las Losowy) to **zespoowy model uczenia maszynowego**, kt贸ry skada si z wielu pojedynczych drzew decyzyjnych. Jego dziaanie opiera si na zasadzie "mdroci tumu". W procesie klasyfikacji, ka偶de drzewo w "lesie" oddaje sw贸j "gos" na dan klas, a ostateczna predykcja modelu to klasa, kt贸ra otrzymaa najwicej gos贸w. Taka agregacja wynik贸w z wielu r贸偶nych drzew sprawia, 偶e model jest znacznie bardziej stabilny i odporny na bdy ni偶 pojedyncze drzewo decyzyjne.

### Pytanie 2: Na czym polega "losowo" w modelu Random Forest? Prosz wymieni dwa g贸wne 藕r贸da losowoci.

> **Odpowied藕:**
> "Losowo" jest kluczowym elementem, kt贸ry sprawia, 偶e drzewa w lesie s zr贸偶nicowane. Istniej dwa g贸wne 藕r贸da losowoci:
> 1.  **Bootstrap Aggregating (Bagging):** Ka偶de drzewo jest trenowane na nieco innym podzbiorze danych treningowych, stworzonym przez **losowanie ze zwracaniem**. Oznacza to, 偶e niekt贸re pr贸bki mog pojawi si w podzbiorze wielokrotnie, a inne wcale.
> 2.  **Losowy wyb贸r cech:** Podczas budowy ka偶dego wza w drzewie, algorytm nie rozwa偶a wszystkich dostpnych cech, ale tylko ich **losowo wybrany podzbi贸r**. To zmusza drzewa do szukania r贸偶nych zale偶noci i zapobiega dominacji jednej, silnej cechy.

### Pytanie 3: Jakie s g贸wne zalety stosowania modelu Random Forest w por贸wnaniu do pojedynczego drzewa decyzyjnego?

> **Odpowied藕:**
> G贸wne zalety to:
> - **Redukcja przeuczenia (overfitting):** Pojedyncze drzewa maj tendencj do "uczenia si na pami" danych treningowych. Agregacja wynik贸w z wielu drzew urednia bdy i sprawia, 偶e model lepiej generalizuje na nowe dane.
> - **Wiksza stabilno i dokadno:** Model jest mniej wra偶liwy na niewielkie zmiany w danych treningowych. Wynik oparty na "gosowaniu" setek drzew jest bardziej wiarygodny.
> - **Wbudowana ocena wa偶noci cech:** Algorytm potrafi oceni, kt贸re cechy miay najwikszy wkad w proces decyzyjny.

### Pytanie 4: Czy model Random Forest jest wra偶liwy na skal cech? Czy wymaga normalizacji/standaryzacji?

> **Odpowied藕:**
> Nie, modele oparte na drzewach decyzyjnych, w tym Random Forest, **nie s wra偶liwe na skal cech**. Dziaaj one na zasadzie podzia贸w (np. "czy wiek > 60?"), a nie na odlegociach, jak np. SVM czy sieci neuronowe. Dlatego standaryzacja **nie jest technicznie wymagana** do poprawnego dziaania algorytmu. W moim projekcie zastosowaem j jednak dla sp贸jnoci z innymi modelami, kt贸re planuj testowa w przyszoci, oraz jako dobr praktyk w budowaniu potok贸w uczenia maszynowego.

### Pytanie 5: Co to jest "out-of-bag error" i jak mo偶na go wykorzysta?

> **Odpowied藕:**
> Dziki technice baggingu, ka偶de drzewo jest trenowane tylko na czci danych (rednio ok. 2/3). Pozostaa 1/3 pr贸bek, kt贸re nie zostay u偶yte do treningu danego drzewa, nazywana jest **pr贸bkami "out-of-bag" (OOB)**. Mo偶emy wykorzysta te pr贸bki do oceny modelu bez potrzeby tworzenia osobnego zbioru walidacyjnego. Dla ka偶dej pr贸bki zbieramy "gosy" tylko od tych drzew, kt贸re nie widziay jej podczas treningu. Bd popeniony na tych pr贸bkach to wanie **"out-of-bag error"**, kt贸ry jest dobrym i bezstronnym estymatorem bdu generalizacji.

### Pytanie 6: Jak Random Forest radzi sobie z problemem niezbalansowanych klas?

> **Odpowied藕:**
> Standardowy Random Forest mo偶e mie tendencj do faworyzowania klasy wikszociowej. Jednak algorytm oferuje skuteczne mechanizmy radzenia sobie z tym problemem. W moim projekcie wykorzystaem hiperparametr `class_weight=\'balanced\'`. Ta opcja automatycznie dostosowuje wagi klas odwrotnie proporcjonalnie do ich liczebnoci. Oznacza to, 偶e bdy popenione na klasie mniejszociowej (w naszym przypadku `DEATH_EVENT = 1`) s "karane" znacznie surowiej, co zmusza model do zwracania na nie wikszej uwagi.

### Pytanie 7: Czym r贸偶ni si Random Forest od innych modeli zespoowych, np. Gradient Boosting (XGBoost)?

> **Odpowied藕:**
> G贸wna r贸偶nica le偶y w sposobie budowania zespou. W **Random Forest** drzewa s budowane **niezale偶nie i r贸wnolegle**. Ka偶de drzewo jest "ekspertem" od czego innego, a na kocu odbywa si gosowanie. W **Gradient Boosting** drzewa s budowane **sekwencyjnie**. Ka偶de kolejne drzewo uczy si na bdach popenionych przez poprzednie. Jest to proces iteracyjny, gdzie model stopniowo "poprawia" swoje saboci. W efekcie Gradient Boosting czsto osiga nieco wy偶sz dokadno, ale jest bardziej podatny na przeuczenie i wymaga staranniejszej optymalizacji.

---

## II. Przygotowanie Danych i Budowa Modelu

### Pytanie 8: Dlaczego do budowy modelu wybrano tylko trzy cechy: wiek, frakcja wyrzutowa i kreatynina w surowicy?

> **Odpowied藕:**
> Decyzja ta opieraa si na dw贸ch filarach: po pierwsze, na wynikach mojej **eksploracyjnej analizy danych (EDA)**, kt贸ra wykazaa, 偶e te trzy cechy maj najsilniejsz korelacj ze zmienn celu. Po drugie, jest to zgodne z **metodyk z publikacji bazowej**, gdzie autorzy, na podstawie analizy prze偶ycia Coksa, r贸wnie偶 zidentyfikowali te cechy jako najwa偶niejsze predyktory. Skupienie si na najsilniejszych sygnaach pozwala zbudowa prostszy, bardziej interpretowalny i czsto bardziej stabilny model.

### Pytanie 9: Jaki by cel podziau danych na zbi贸r treningowy i testowy?

> **Odpowied藕:**
> Celem tego podziau jest **rzetelna ocena zdolnoci generalizacji modelu**. Zbi贸r treningowy (80% danych) su偶y do "nauczenia" modelu wzorc贸w. Zbi贸r testowy (20% danych) jest trzymany "w ukryciu" i u偶ywany dopiero na samym kocu. Ocena na danych, kt贸rych model nigdy wczeniej nie widzia, symuluje jego dziaanie w rzeczywistych warunkach i pozwala sprawdzi, czy nie nauczy si on po prostu na pami zbioru treningowego (overfitting).

### Pytanie 10: Co to jest stratyfikacja i dlaczego zostaa u偶yta podczas podziau danych?

> **Odpowied藕:**
> **Stratyfikacja** to technika, kt贸ra zapewnia, 偶e proporcje klas w zbiorze danych s zachowane po podziale. W naszym przypadku, klasa `DEATH_EVENT` jest niezbalansowana (68% prze偶yo, 32% zmaro). Bez stratyfikacji mogoby si zdarzy, 偶e w wyniku losowego podziau, w zbiorze testowym znalazoby si np. bardzo mao przypadk贸w zgon贸w, co sprawioby, 偶e ocena modelu byaby niewiarygodna. Dziki stratyfikacji, zar贸wno w zbiorze treningowym, jak i testowym, mamy taki sam procentowy rozkad klas jak w oryginalnym zbiorze.

### Pytanie 11: Jakie konkretne kroki obejmowa potok (pipeline) uczenia maszynowego w Pana/Pani projekcie?

> **Odpowied藕:**
> M贸j potok skada si z nastpujcych krok贸w:
> 1.  **Wczytanie i selekcja cech:** Wczytanie danych i wyb贸r trzech kluczowych predyktor贸w.
> 2.  **Podzia danych:** Podzia na zbi贸r treningowy (80%) i testowy (20%) z stratyfikacj.
> 3.  **Standaryzacja:** Dopasowanie `StandardScaler` na zbiorze treningowym i transformacja obu zbior贸w.
> 4.  **Optymalizacja hiperparametr贸w:** U偶ycie `RandomizedSearchCV` z 5-krotn walidacj krzy偶ow na zbiorze treningowym w celu znalezienia najlepszych parametr贸w dla modelu.
> 5.  **Trening finalnego modelu:** Wytrenowanie modelu Random Forest z optymalnymi parametrami na caym zbiorze treningowym.
> 6.  **Ewaluacja:** Ocena modelu na zbiorze testowym za pomoc r贸偶nych metryk (F1, Recall, AUC itp.).

### Pytanie 12: Jakie byy wymiary zbioru treningowego i testowego?

> **Odpowied藕:**
> Oryginalny zbi贸r liczy 299 pr贸bek. Po podziale 80/20, zbi贸r treningowy skada si z **239 pr贸bek**, a zbi贸r testowy z **60 pr贸bek**.

### Pytanie 13: Czy model by trenowany na danych surowych czy przeskalowanych?

> **Odpowied藕:**
> Model by trenowany na danych **przeskalowanych** za pomoc `StandardScaler`. Chocia偶 Random Forest nie wymaga skalowania, jest to dobra praktyka, kt贸ra zapewnia sp贸jno i uatwia por贸wnywanie z innymi modelami, kt贸re planuj testowa, a kt贸re s wra偶liwe na skal cech (np. sieci neuronowe).

### Pytanie 14: Jakie oprogramowanie i biblioteki zostay wykorzystane do implementacji modelu?

> **Odpowied藕:**
> Cay proces zosta zaimplementowany w jzyku **Python** przy u偶yciu biblioteki **Scikit-learn**. `RandomForestClassifier` posu偶y do budowy modelu, `train_test_split` do podziau danych, `StandardScaler` do normalizacji, a `RandomizedSearchCV` do optymalizacji hiperparametr贸w. Do wizualizacji wynik贸w u偶yem bibliotek **Matplotlib** i **Seaborn**.

---

## III. Optymalizacja i Walidacja

### Pytanie 15: Co to s hiperparametry i dlaczego ich optymalizacja jest wa偶na?

> **Odpowied藕:**
> Hiperparametry to zewntrzne parametry konfiguracyjne modelu, kt贸re nie s uczone z danych, ale musz by ustawione przed treningiem. Przykady to liczba drzew w lesie czy maksymalna gboko drzewa. Ich optymalizacja jest kluczowa, poniewa偶 odpowiednie wartoci mog znaczco **poprawi skuteczno modelu**, podczas gdy ze ustawienia mog prowadzi do przeuczenia lub niedouczenia. Celem optymalizacji jest znalezienie "zotego rodka", kt贸ry zapewni najlepsz zdolno generalizacji.

### Pytanie 16: Czym r贸偶ni si `RandomizedSearchCV` od `GridSearchCV` i dlaczego wybrano to pierwsze?

> **Odpowied藕:**
> `GridSearchCV` testuje **wszystkie mo偶liwe kombinacje** zdefiniowanych hiperparametr贸w. Jest to bardzo dokadne, ale przy du偶ej liczbie parametr贸w staje si niezwykle czasochonne. `RandomizedSearchCV` testuje tylko **losowo wybran liczb kombinacji** (w moim przypadku 100). Wybraem `RandomizedSearchCV`, poniewa偶 jest znacznie szybszy i czsto znajduje r贸wnie dobre (lub prawie tak dobre) wyniki, co `GridSearchCV`, zwaszcza gdy niekt贸re hiperparametry maj mniejszy wpyw na wynik. Pozwolio mi to przeszuka szersz przestrze parametr贸w w rozsdnym czasie.

### Pytanie 17: Co to jest walidacja krzy偶owa (cross-validation) i dlaczego jest lepsza ni偶 pojedynczy podzia walidacyjny?

> **Odpowied藕:**
> Walidacja krzy偶owa to technika oceny modelu, kt贸ra polega na wielokrotnym podziale danych na zbiory treningowe i walidacyjne. W **k-krotnej walidacji krzy偶owej** (u mnie k=5), dane s dzielone na k czci. Model jest trenowany k razy, za ka偶dym razem na k-1 czciach, a testowany na pozostaej. Wyniki s uredniane. Jest to metoda znacznie bardziej **wiarygodna i stabilna** ni偶 pojedynczy podzia, poniewa偶 ocena nie zale偶y od "szczcia" w losowym podziale danych. Daje nam lepsze pojcie o tym, jak model bdzie si zachowywa na zupenie nowych danych.

### Pytanie 18: Jakie byy najlepsze hiperparametry znalezione dla Pana/Pani modelu?

> **Odpowied藕:**
> Najlepsze znalezione hiperparametry to: 100 drzew (`n_estimators`), minimalna liczba pr贸bek do podziau r贸wna 5 (`min_samples_split`), minimalna liczba pr贸bek w liciu r贸wna 8 (`min_samples_leaf`) oraz, co bardzo wa偶ne, u偶ycie wag klas (`class_weight=\'balanced\'`). Ustawienie `min_samples_leaf` na 8 pomaga w regularyzacji, zapobiegajc tworzeniu zbyt skomplikowanych drzew, kt贸re mogyby si przeuczy.

### Pytanie 19: Jak metryk wybrano do optymalizacji w `RandomizedSearchCV` i dlaczego?

> **Odpowied藕:**
> Do optymalizacji wybrano metryk **F1-score**. W przypadku niezbalansowanych danych, optymalizacja pod ktem samej dokadnoci (accuracy) byaby bdem, poniewa偶 model m贸gby osign wysoki wynik, ignorujc klas mniejszociow. F1-score jest redni harmoniczn precyzji i czuoci, co czyni go dobrym, zbalansowanym wska藕nikiem skutecznoci, gdy zale偶y nam na poprawnym klasyfikowaniu obu klas.

---

## IV. Ewaluacja i Metryki

### Pytanie 20: Prosz wyjani, co oznaczaj metryki: Precision, Recall i F1-score.

> **Odpowied藕:**
> - **Precision (Precyzja):** M贸wi nam, **jaki procent predykcji pozytywnych by poprawny**. W naszym kontekcie: "Spor贸d wszystkich pacjent贸w, kt贸rych model oznaczy jako zagro偶onych zgonem, ilu faktycznie zmaro?". Wysoka precyzja oznacza mao faszywych alarm贸w.
> - **Recall (Czuo):** M贸wi nam, **jaki procent wszystkich faktycznych przypadk贸w pozytywnych model poprawnie wykry**. W naszym kontekcie: "Spor贸d wszystkich pacjent贸w, kt贸rzy faktycznie zmarli, ilu model poprawnie zidentyfikowa?". Wysoka czuo oznacza mao "przegapionych" przypadk贸w.
> - **F1-score:** Jest to **rednia harmoniczna precyzji i czuoci**. Stanowi kompromis midzy tymi dwiema metrykami i jest szczeg贸lnie u偶yteczna, gdy mamy do czynienia z niezbalansowanymi klasami.

### Pytanie 21: W Pana/Pani modelu Recall (89.5%) jest znacznie wy偶szy ni偶 Precision (54.8%). Co to oznacza w kontekcie medycznym i czy jest to po偶dany wynik?

> **Odpowied藕:**
> Tak, w tym konkretnym problemie medycznym jest to **bardzo po偶dany wynik**. Wysoki Recall oznacza, 偶e nasz model jest **bardzo skuteczny w identyfikowaniu pacjent贸w, kt贸rzy faktycznie s w grupie wysokiego ryzyka** (przegapi tylko 2 z 19 takich pacjent贸w w zbiorze testowym). Ni偶sza precyzja oznacza, 偶e model generuje pewn liczb "faszywych alarm贸w" (14 pacjent贸w oznaczonych jako zagro偶eni, kt贸rzy prze偶yli). Z klinicznego punktu widzenia, konsekwencje **przegapienia pacjenta wysokiego ryzyka (False Negative)** s znacznie powa偶niejsze ni偶 konsekwencje **faszywego alarmu (False Positive)**, kt贸ry co najwy偶ej skieruje pacjenta na dodatkowe badania. Dlatego priorytetyzacja Recall jest tutaj uzasadniona.

### Pytanie 22: Co przedstawia macierz pomyek (confusion matrix) i jak j interpretowa?

> **Odpowied藕:**
> Macierz pomyek to tabela, kt贸ra podsumowuje wyniki klasyfikacji. Wiersze reprezentuj prawdziwe klasy, a kolumny - klasy przewidziane przez model. W naszej macierzy:
> - **True Positives (TP=17):** Poprawnie zidentyfikowani zmarli.
> - **True Negatives (TN=27):** Poprawnie zidentyfikowani 偶yjcy.
> - **False Positives (FP=14):** 呕yjcy, bdnie oznaczeni jako zmarli ("faszywy alarm").
> - **False Negatives (FN=2):** Zmarli, bdnie oznaczeni jako 偶yjcy ("przegapienie").

Macierz ta daje peny obraz dziaania modelu, znacznie bardziej szczeg贸owy ni偶 pojedyncza metryka jak dokadno.

### Pytanie 23: Co to jest krzywa ROC i co oznacza pole pod ni (AUC)?

> **Odpowied藕:**
> Krzywa ROC (Receiver Operating Characteristic) to wykres, kt贸ry pokazuje zdolno modelu do odr贸偶niania klas. Ilustruje ona kompromis midzy **True Positive Rate (Recall)** a **False Positive Rate** przy r贸偶nych progach decyzyjnych. **Pole pod krzyw (AUC - Area Under the Curve)** jest pojedyncz liczb podsumowujc jej jako. AUC r贸wne 1.0 oznacza idealny klasyfikator, a 0.5 - klasyfikator losowy. Nasz wynik **AUC = 0.7689** oznacza, 偶e model ma dobr, znacznie lepsz od losowej, zdolno do rozr贸偶niania pacjent贸w, kt贸rzy prze偶yj, od tych, kt贸rzy umr.

### Pytanie 24: Dlaczego opr贸cz krzywej ROC analizuje si r贸wnie偶 krzyw Precision-Recall?

> **Odpowied藕:**
> Krzywa ROC mo偶e by zbyt optymistyczna w przypadku silnie niezbalansowanych danych, poniewa偶 uwzgldnia True Negatives, kt贸rych jest bardzo du偶o. Krzywa Precision-Recall, kt贸ra pokazuje kompromis midzy precyzj a czuoci, jest czsto uwa偶ana za bardziej informatywn w takich przypadkach. Skupia si ona na wydajnoci modelu w odniesieniu do rzadkiej, pozytywnej klasy, co jest kluczowe w naszym problemie. Nasz wynik **AUC-PR = 0.6961**, znacznie powy偶ej linii bazowej, potwierdza, 偶e model jest skuteczny.

### Pytanie 25: Wyniki z walidacji krzy偶owej (CV F1-score: 0.6661 卤 0.0857) s nieco ni偶sze ni偶 na zbiorze testowym (F1-score: 0.6800). Co to oznacza?

> **Odpowied藕:**
> Wyniki s bardzo zbli偶one, co jest dobrym znakiem. Walidacja krzy偶owa daje bardziej urednion i realistyczn ocen wydajnoci modelu. Niewielkie odchylenie standardowe (卤0.0857) wiadczy o tym, 偶e model jest **stabilny** i jego wyniki nie zale偶 mocno od konkretnego podziau danych. Fakt, 偶e wynik na zbiorze testowym jest podobny do redniej z CV, potwierdza, 偶e nasz model dobrze generalizuje i nie jest przeuczony.

### Pytanie 26: Czy uzyskane wyniki s satysfakcjonujce w por贸wnaniu do wynik贸w z publikacji bazowej?

> **Odpowied藕:**
> Publikacja bazowa podaje F1-score na poziomie 88.37% dla modelu SVM, podczas gdy nasz Random Forest osign 68.00%. R贸偶nica ta mo偶e wynika z kilku czynnik贸w: u偶ylimy tylko 3 cech, podczas gdy autorzy mogli u偶y ich wicej; mogli te偶 zastosowa inn metodologi preprocessingu. Nale偶y jednak podkreli, 偶e naszym celem bya **reprodukcja metodyki i zbudowanie klinicznie u偶ytecznego modelu**, a nie lepe d偶enie do jak najwy偶szego wyniku. Nasz model, z **Recall na poziomie 89.5%**, jest bardzo wartociowy z praktycznego punktu widzenia, nawet jeli jego F1-score jest ni偶szy.

---

## V. Interpretacja Wynik贸w i Wnioski

### Pytanie 27: Co oznacza mechanizm "feature importance" w Random Forest i jakie byy jego wyniki w Pana/Pani modelu?

> **Odpowied藕:**
> "Feature importance" to mechanizm, kt贸ry ocenia, jak bardzo ka偶da cecha przyczynia si do poprawy czystoci podzia贸w (redukcji zanieczyszczenia) we wszystkich drzewach w lesie. Innymi sowy, m贸wi nam, kt贸re cechy byy najczciej i najskuteczniej wykorzystywane przez model do podejmowania decyzji. W moim modelu wyniki byy jednoznaczne:
> 1.  **Kreatynina w surowicy (46.07%):** Zdecydowanie najwa偶niejsza cecha.
> 2.  **Frakcja wyrzutowa (33.76%):** Druga co do wa偶noci.
> 3.  **Wiek (20.17%):** Trzecia, ale wci偶 istotna.

### Pytanie 28: Jakie praktyczne, kliniczne wnioski mo偶na wycign z faktu, 偶e kreatynina w surowicy jest najwa偶niejsz cech?

> **Odpowied藕:**
> Wniosek jest taki, 偶e **stan nerek jest kluczowym czynnikiem prognostycznym** u pacjent贸w z niewydolnoci serca. Potwierdza to istnienie tzw. **zespou sercowo-nerkowego**, gdzie niewydolno jednego organu napdza niewydolno drugiego. Z praktycznego punktu widzenia oznacza to, 偶e monitorowanie funkcji nerek (poprzez pomiar kreatyniny) powinno by absolutnym priorytetem w opiece nad tymi pacjentami, a ka偶dy wzrost jej poziomu powinien by traktowany jako powa偶ny sygna alarmowy.

### Pytanie 29: Czy zbudowany model m贸gby by u偶yty w praktyce klinicznej? Jakie s jego ograniczenia?

> **Odpowied藕:**
> Model wykazuje du偶y potencja jako **narzdzie wspomagajce decyzje kliniczne**, g贸wnie dziki bardzo wysokiej czuoci (Recall). M贸gby su偶y jako system wczesnego ostrzegania, identyfikujcy pacjent贸w, kt贸rzy wymagaj pilniejszej uwagi lub intensywniejszego leczenia. G贸wne ograniczenia to:
> - **May zbi贸r danych:** Model zosta wytrenowany na tylko 299 pacjentach, co ogranicza jego zdolno generalizacji. Wymaga walidacji na znacznie wikszej i bardziej zr贸偶nicowanej populacji.
> - **Ograniczona liczba cech:** U偶ycie tylko 3 cech upraszcza model, ale mo偶e pomija inne istotne czynniki.
> - **Brak walidacji zewntrznej:** Model nie by testowany na danych z innego szpitala czy kraju.

### Pytanie 30: Jakie s Pana/Pani rekomendacje dotyczce dalszych krok贸w w rozwijaniu tego modelu?

> **Odpowied藕:**
> Rekomenduj nastpujce kroki:
> 1.  **Feature Engineering:** Stworzenie nowych cech (np. wska藕nika sercowo-nerkowego) i przetestowanie, czy dodanie innych cech z oryginalnego zbioru (np. sodu, cinienia krwi) poprawi wyniki.
> 2.  **Testowanie innych modeli:** Por贸wnanie Random Forest z innymi algorytmami, takimi jak XGBoost, LightGBM czy SVM, aby znale藕 najskuteczniejsze podejcie.
> 3.  **Budowa modeli sieci neuronowych (MLP):** Sprawdzenie, czy bardziej zo偶one modele s w stanie uchwyci nieliniowe zale偶noci, kt贸rych Random Forest m贸g nie dostrzec.
> 4.  **Analiza prze偶ycia:** Zastosowanie modeli specyficznych dla analizy prze偶ycia (np. DeepSurv), kt贸re modeluj nie tylko to, CZY pacjent umrze, ale r贸wnie偶 KIEDY to mo偶e nastpi.
